# ğŸ¤– Ollama AI æœåŠ¡ä½¿ç”¨æŒ‡å—

## âœ… å·²å®Œæˆçš„åŠŸèƒ½

1. **Ollama é›†æˆ** - æœ¬åœ° LLM æ”¯æŒ
2. **AI å¯¹è¯** - å®Œæ•´çš„å¯¹è¯åŠŸèƒ½
3. **æµå¼å“åº”** - å®æ—¶æ‰“å­—æ•ˆæœ
4. **æ¨¡å‹ç®¡ç†** - åˆ—å‡ºå’Œæ£€æŸ¥å¯ç”¨æ¨¡å‹
5. **å¤šç§ AI åŠ©æ‰‹** - ä»£ç å®¡æŸ¥ã€DevOpsã€æˆæœ¬ä¼˜åŒ–

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… Ollama

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# æˆ–è€…è®¿é—® https://ollama.com/download ä¸‹è½½å®‰è£…åŒ…
```

### 2. æ‹‰å–æ¨èæ¨¡å‹

```bash
# è½»é‡çº§æ¨¡å‹ï¼ˆ3B å‚æ•°ï¼Œæ¨èï¼‰
ollama pull llama3.2:3b

# ä»£ç ä¸“ç”¨æ¨¡å‹
ollama pull codellama:7b

# é€šç”¨æ¨¡å‹ï¼ˆæ›´å¼ºå¤§ï¼‰
ollama pull llama3.2:latest

# ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
ollama pull qwen2.5:7b
```

### 3. å¯åŠ¨ Ollama æœåŠ¡

```bash
# Ollama ä¼šè‡ªåŠ¨åœ¨åå°è¿è¡Œ
# éªŒè¯æœåŠ¡
curl http://localhost:11434/api/tags

# æˆ–è€…æµ‹è¯•å¯¹è¯
ollama run llama3.2:3b
```

### 4. é…ç½®ç¯å¢ƒå˜é‡

```bash
# .env æ–‡ä»¶å·²ç»é…ç½®å¥½äº†
OLLAMA_HOST=http://localhost:11434
```

### 5. å¯åŠ¨åº”ç”¨

```bash
bun run dev
```

---

## ğŸ“¡ API ä½¿ç”¨

### åˆ›å»º AI åŠ©æ‰‹

```typescript
// ä½¿ç”¨ tRPC Client
const assistant = await client.aiAssistants.create.mutate({
  organizationId: 'org-uuid', // å¯é€‰
  name: 'DevOps åŠ©æ‰‹',
  type: 'devops-engineer',
  modelConfig: {
    provider: 'ollama',
    model: 'llama3.2:3b', // æˆ– 'codellama:7b', 'qwen2.5:7b'
    temperature: 0.7,
  },
  systemPrompt: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ DevOps å·¥ç¨‹å¸ˆã€‚
ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·ï¼š
1. ä¼˜åŒ– CI/CD æµç¨‹
2. è§£å†³éƒ¨ç½²é—®é¢˜
3. æä¾›æœ€ä½³å®è·µå»ºè®®
4. åˆ†æç³»ç»Ÿæ€§èƒ½

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚`,
  isActive: true,
})
```

### ä¸ AI å¯¹è¯

```typescript
// æ™®é€šå¯¹è¯
const response = await client.aiAssistants.chat.mutate({
  assistantId: assistant.id,
  message: 'å¦‚ä½•ä¼˜åŒ– Docker é•œåƒå¤§å°ï¼Ÿ',
  context: {
    currentSize: '1.2GB',
    baseImage: 'node:20',
  },
})

console.log(response.message)
```

### æ£€æŸ¥ Ollama çŠ¶æ€

```typescript
const status = await client.aiAssistants.checkOllamaStatus.query()

console.log(status)
// {
//   available: true,
//   modelCount: 3,
//   models: ['llama3.2:3b', 'codellama:7b', 'qwen2.5:7b']
// }
```

### åˆ—å‡ºå¯ç”¨æ¨¡å‹

```typescript
const models = await client.aiAssistants.listOllamaModels.query()

models.forEach((model) => {
  console.log(`${model.name} - ${(model.size / 1024 / 1024 / 1024).toFixed(2)} GB`)
})
```

---

## ğŸ¨ å‰ç«¯ç¤ºä¾‹

### React å¯¹è¯ç»„ä»¶

```typescript
import { useState } from 'react'
import { trpc } from './trpc'

function AiChat({ assistantId }: { assistantId: string }) {
  const [message, setMessage] = useState('')
  const [messages, setMessages] = useState<Array<{ role: string; content: string }>>([])
  const [loading, setLoading] = useState(false)

  const chatMutation = trpc.aiAssistants.chat.useMutation()

  const handleSend = async () => {
    if (!message.trim()) return

    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    const userMessage = { role: 'user', content: message }
    setMessages((prev) => [...prev, userMessage])
    setMessage('')
    setLoading(true)

    try {
      // è°ƒç”¨ AI
      const response = await chatMutation.mutateAsync({
        assistantId,
        message,
      })

      // æ·»åŠ  AI å“åº”
      setMessages((prev) => [
        ...prev,
        { role: 'assistant', content: response.message },
      ])
    } catch (error) {
      console.error('Chat error:', error)
      alert('å¯¹è¯å¤±è´¥ï¼š' + error.message)
    } finally {
      setLoading(false)
    }
  }

  return (
    <div className="chat-container">
      {/* æ¶ˆæ¯åˆ—è¡¨ */}
      <div className="messages">
        {messages.map((msg, i) => (
          <div key={i} className={`message ${msg.role}`}>
            <strong>{msg.role === 'user' ? 'ä½ ' : 'AI'}:</strong>
            <p>{msg.content}</p>
          </div>
        ))}
        {loading && <div className="loading">AI æ­£åœ¨æ€è€ƒ...</div>}
      </div>

      {/* è¾“å…¥æ¡† */}
      <div className="input-area">
        <input
          type="text"
          value={message}
          onChange={(e) => setMessage(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && handleSend()}
          placeholder="è¾“å…¥æ¶ˆæ¯..."
          disabled={loading}
        />
        <button onClick={handleSend} disabled={loading || !message.trim()}>
          å‘é€
        </button>
      </div>
    </div>
  )
}
```

### æ£€æŸ¥ Ollama çŠ¶æ€

```typescript
function OllamaStatus() {
  const { data: status } = trpc.aiAssistants.checkOllamaStatus.useQuery()

  if (!status) return <div>æ£€æŸ¥ä¸­...</div>

  if (!status.available) {
    return (
      <div className="alert alert-error">
        âŒ Ollama æœåŠ¡ä¸å¯ç”¨
        <p>{status.error}</p>
        <a href="https://ollama.com/download" target="_blank">
          ä¸‹è½½ Ollama
        </a>
      </div>
    )
  }

  return (
    <div className="alert alert-success">
      âœ… Ollama æœåŠ¡æ­£å¸¸
      <p>å·²å®‰è£… {status.modelCount} ä¸ªæ¨¡å‹</p>
      <ul>
        {status.models.map((model) => (
          <li key={model}>{model}</li>
        ))}
      </ul>
    </div>
  )
}
```

---

## ğŸ¯ æ¨èæ¨¡å‹

### 1. llama3.2:3b (æ¨èæ–°æ‰‹)
- **å¤§å°**: 2GB
- **é€Ÿåº¦**: éå¸¸å¿«
- **ç”¨é€”**: é€šç”¨å¯¹è¯ã€ä»£ç è§£é‡Š
- **ä¼˜ç‚¹**: èµ„æºå ç”¨å°‘ï¼Œå“åº”å¿«

```bash
ollama pull llama3.2:3b
```

### 2. codellama:7b (ä»£ç ä¸“ç”¨)
- **å¤§å°**: 3.8GB
- **é€Ÿåº¦**: å¿«
- **ç”¨é€”**: ä»£ç å®¡æŸ¥ã€ä»£ç ç”Ÿæˆã€Bug ä¿®å¤
- **ä¼˜ç‚¹**: ä¸“é—¨é’ˆå¯¹ä»£ç ä¼˜åŒ–

```bash
ollama pull codellama:7b
```

### 3. qwen2.5:7b (ä¸­æ–‡ä¼˜åŒ–)
- **å¤§å°**: 4.7GB
- **é€Ÿåº¦**: ä¸­ç­‰
- **ç”¨é€”**: ä¸­æ–‡å¯¹è¯ã€ä¸­æ–‡ä»£ç æ³¨é‡Š
- **ä¼˜ç‚¹**: ä¸­æ–‡ç†è§£èƒ½åŠ›å¼º

```bash
ollama pull qwen2.5:7b
```

### 4. llama3.2:latest (æœ€å¼ºå¤§)
- **å¤§å°**: 26GB
- **é€Ÿåº¦**: è¾ƒæ…¢
- **ç”¨é€”**: å¤æ‚æ¨ç†ã€æ·±åº¦åˆ†æ
- **ä¼˜ç‚¹**: èƒ½åŠ›æœ€å¼º

```bash
ollama pull llama3.2:latest
```

---

## ğŸ¨ é¢„è®¾ AI åŠ©æ‰‹

### 1. ä»£ç å®¡æŸ¥åŠ©æ‰‹

```typescript
{
  name: 'ä»£ç å®¡æŸ¥åŠ©æ‰‹',
  type: 'code-reviewer',
  modelConfig: {
    provider: 'ollama',
    model: 'codellama:7b',
    temperature: 0.3, // ä½æ¸©åº¦ï¼Œæ›´ç²¾ç¡®
  },
  systemPrompt: `ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚
å®¡æŸ¥ä»£ç æ—¶ï¼Œè¯·å…³æ³¨ï¼š
1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
2. æ½œåœ¨çš„ Bug å’Œå®‰å…¨é—®é¢˜
3. æ€§èƒ½ä¼˜åŒ–å»ºè®®
4. æœ€ä½³å®è·µ

è¯·æä¾›å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ã€‚`
}
```

### 2. DevOps å·¥ç¨‹å¸ˆ

```typescript
{
  name: 'DevOps åŠ©æ‰‹',
  type: 'devops-engineer',
  modelConfig: {
    provider: 'ollama',
    model: 'llama3.2:3b',
    temperature: 0.7,
  },
  systemPrompt: `ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ DevOps å·¥ç¨‹å¸ˆã€‚
ä½ æ“…é•¿ï¼š
1. CI/CD æµç¨‹ä¼˜åŒ–
2. Docker å’Œ Kubernetes
3. ç›‘æ§å’Œæ—¥å¿—åˆ†æ
4. è‡ªåŠ¨åŒ–éƒ¨ç½²

è¯·æä¾›å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚`
}
```

### 3. æˆæœ¬ä¼˜åŒ–ä¸“å®¶

```typescript
{
  name: 'æˆæœ¬ä¼˜åŒ–åŠ©æ‰‹',
  type: 'cost-optimizer',
  modelConfig: {
    provider: 'ollama',
    model: 'llama3.2:3b',
    temperature: 0.5,
  },
  systemPrompt: `ä½ æ˜¯ä¸€ä¸ªäº‘æˆæœ¬ä¼˜åŒ–ä¸“å®¶ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ
2. è¯†åˆ«æµªè´¹å’Œä¼˜åŒ–æœºä¼š
3. æä¾›å…·ä½“çš„èŠ‚çœå»ºè®®
4. è®¡ç®—é¢„æœŸèŠ‚çœé‡‘é¢

è¯·æä¾›æ•°æ®é©±åŠ¨çš„å»ºè®®ã€‚`
}
```

---

## ğŸ”§ Ollama ç®¡ç†

### æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹

```bash
ollama list
```

### åˆ é™¤æ¨¡å‹

```bash
ollama rm llama3.2:3b
```

### æ›´æ–°æ¨¡å‹

```bash
ollama pull llama3.2:3b
```

### æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯

```bash
ollama show llama3.2:3b
```

### åœæ­¢ Ollama æœåŠ¡

```bash
# macOS
brew services stop ollama

# Linux
systemctl stop ollama
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | é€Ÿåº¦ | è´¨é‡ | æ¨èç”¨é€” |
|------|------|------|------|---------|
| llama3.2:3b | 2GB | âš¡âš¡âš¡âš¡âš¡ | â­â­â­ | å¿«é€Ÿå¯¹è¯ |
| codellama:7b | 3.8GB | âš¡âš¡âš¡âš¡ | â­â­â­â­ | ä»£ç ç›¸å…³ |
| qwen2.5:7b | 4.7GB | âš¡âš¡âš¡ | â­â­â­â­ | ä¸­æ–‡å¯¹è¯ |
| llama3.2:latest | 26GB | âš¡âš¡ | â­â­â­â­â­ | å¤æ‚ä»»åŠ¡ |

---

## ğŸ› æ•…éšœæ’æŸ¥

### Ollama æœåŠ¡æœªè¿è¡Œ

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:11434/api/tags

# å¦‚æœå¤±è´¥ï¼Œå¯åŠ¨æœåŠ¡
ollama serve

# æˆ–è€…é‡å¯
brew services restart ollama  # macOS
systemctl restart ollama      # Linux
```

### æ¨¡å‹æœªæ‰¾åˆ°

```bash
# åˆ—å‡ºå·²å®‰è£…æ¨¡å‹
ollama list

# æ‹‰å–ç¼ºå¤±çš„æ¨¡å‹
ollama pull llama3.2:3b
```

### å“åº”å¤ªæ…¢

1. **ä½¿ç”¨æ›´å°çš„æ¨¡å‹**: llama3.2:3b è€Œä¸æ˜¯ llama3.2:latest
2. **é™ä½æ¸©åº¦**: temperature è®¾ç½®ä¸º 0.3-0.5
3. **å‡å°‘ä¸Šä¸‹æ–‡**: ä¸è¦å‘é€å¤ªé•¿çš„æ¶ˆæ¯

### å†…å­˜ä¸è¶³

```bash
# æŸ¥çœ‹æ¨¡å‹å¤§å°
ollama list

# åˆ é™¤ä¸ç”¨çš„å¤§æ¨¡å‹
ollama rm llama3.2:latest

# åªä¿ç•™å°æ¨¡å‹
ollama pull llama3.2:3b
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹

- **å¿«é€ŸåŸå‹**: llama3.2:3b
- **ä»£ç ä»»åŠ¡**: codellama:7b
- **ä¸­æ–‡åœºæ™¯**: qwen2.5:7b
- **å¤æ‚æ¨ç†**: llama3.2:latest

### 2. ä¼˜åŒ– System Prompt

```typescript
// âŒ ä¸å¥½çš„ prompt
systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹'

// âœ… å¥½çš„ prompt
systemPrompt: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ DevOps å·¥ç¨‹å¸ˆã€‚
ä½ çš„èŒè´£æ˜¯ï¼š
1. åˆ†æ CI/CD é—®é¢˜
2. æä¾›å…·ä½“çš„è§£å†³æ–¹æ¡ˆ
3. è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ

å›ç­”æ—¶è¯·ï¼š
- ä½¿ç”¨ç®€æ´çš„è¯­è¨€
- æä¾›ä»£ç ç¤ºä¾‹
- è¯´æ˜åŸå› å’Œå½±å“`
```

### 3. ä½¿ç”¨ä¸Šä¸‹æ–‡

```typescript
// æä¾›ç›¸å…³ä¸Šä¸‹æ–‡
await client.aiAssistants.chat.mutate({
  assistantId,
  message: 'å¦‚ä½•ä¼˜åŒ–è¿™ä¸ªéƒ¨ç½²ï¼Ÿ',
  context: {
    currentStrategy: 'rolling',
    replicas: 3,
    updateTime: '5 minutes',
    errorRate: '2%',
  },
})
```

### 4. æ¸©åº¦è®¾ç½®

```typescript
// ä»£ç ç”Ÿæˆ - ä½æ¸©åº¦ï¼ˆæ›´ç²¾ç¡®ï¼‰
temperature: 0.3

// åˆ›æ„å†™ä½œ - é«˜æ¸©åº¦ï¼ˆæ›´å¤šæ ·ï¼‰
temperature: 0.9

// é€šç”¨å¯¹è¯ - ä¸­ç­‰æ¸©åº¦
temperature: 0.7
```

---

## ğŸ“ å­¦ä¹ èµ„æº

- [Ollama å®˜ç½‘](https://ollama.com/)
- [Ollama æ–‡æ¡£](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [æ¨¡å‹åº“](https://ollama.com/library)
- [Llama 3.2 ä»‹ç»](https://ollama.com/library/llama3.2)
- [CodeLlama ä»‹ç»](https://ollama.com/library/codellama)

---

## âœ… æµ‹è¯•æ¸…å•

- [ ] Ollama æœåŠ¡è¿è¡Œæ­£å¸¸
- [ ] è‡³å°‘å®‰è£…ä¸€ä¸ªæ¨¡å‹
- [ ] å¯ä»¥åˆ›å»º AI åŠ©æ‰‹
- [ ] å¯ä»¥è¿›è¡Œå¯¹è¯
- [ ] çŠ¶æ€æ£€æŸ¥ API å·¥ä½œ
- [ ] æ¨¡å‹åˆ—è¡¨ API å·¥ä½œ
- [ ] å“åº”é€Ÿåº¦å¯æ¥å—
- [ ] ä¸­æ–‡å¯¹è¯æ­£å¸¸

---

## ğŸš€ ä¸‹ä¸€æ­¥

1. **å°è¯•ä¸åŒæ¨¡å‹** - æ‰¾åˆ°æœ€é€‚åˆä½ çš„
2. **ä¼˜åŒ– Prompt** - æé«˜å›ç­”è´¨é‡
3. **æ·»åŠ å¯¹è¯å†å²** - å®ç°å¤šè½®å¯¹è¯
4. **é›†æˆåˆ°å·¥ä½œæµ** - è‡ªåŠ¨ä»£ç å®¡æŸ¥ã€éƒ¨ç½²å»ºè®®

---

éœ€è¦å¸®åŠ©ï¼Ÿ
```bash
# æŸ¥çœ‹ Ollama æ—¥å¿—
ollama logs

# æµ‹è¯•å¯¹è¯
ollama run llama3.2:3b
```
