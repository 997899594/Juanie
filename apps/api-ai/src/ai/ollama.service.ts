/**
 * ğŸš€ Juanie AI - Ollamaæœ¬åœ°AIæœåŠ¡
 * é›†æˆå’Œç®¡ç†æœ¬åœ°Ollama AIæ¨¡å‹
 */

import { Injectable, Logger, OnModuleInit, OnModuleDestroy } from '@nestjs/common';
import { EventEmitter2 } from '@nestjs/event-emitter';
import { z } from 'zod';
import { 
  withTimeout, 
  retry, 
  CONSTANTS,
  getEnvVar,
  getBooleanEnvVar,
  type AITask,
  type AIResult,
} from '../core';

// ============================================================================
// Ollamaé…ç½®Schema
// ============================================================================

export const OllamaModelSchema = z.object({
  name: z.string(),
  tag: z.string().default('latest'),
  size: z.number().optional(),
  digest: z.string().optional(),
  modified_at: z.string().optional(),
  details: z.object({
    parent_model: z.string().optional(),
    format: z.string().optional(),
    family: z.string().optional(),
    families: z.array(z.string()).optional(),
    parameter_size: z.string().optional(),
    quantization_level: z.string().optional(),
  }).optional(),
});

export const OllamaGenerateRequestSchema = z.object({
  model: z.string(),
  prompt: z.string(),
  suffix: z.string().optional(),
  images: z.array(z.string()).optional(), // Base64ç¼–ç çš„å›¾ç‰‡
  format: z.enum(['json']).optional(),
  options: z.object({
    temperature: z.number().min(0).max(2).optional(),
    top_k: z.number().optional(),
    top_p: z.number().min(0).max(1).optional(),
    repeat_penalty: z.number().optional(),
    seed: z.number().optional(),
    num_predict: z.number().optional(),
    stop: z.array(z.string()).optional(),
  }).optional(),
  system: z.string().optional(),
  template: z.string().optional(),
  context: z.array(z.number()).optional(),
  stream: z.boolean().default(false),
  raw: z.boolean().default(false),
  keep_alive: z.string().optional(),
});

export const OllamaGenerateResponseSchema = z.object({
  model: z.string(),
  created_at: z.string(),
  response: z.string(),
  done: z.boolean(),
  context: z.array(z.number()).optional(),
  total_duration: z.number().optional(),
  load_duration: z.number().optional(),
  prompt_eval_count: z.number().optional(),
  prompt_eval_duration: z.number().optional(),
  eval_count: z.number().optional(),
  eval_duration: z.number().optional(),
});

export const OllamaChatMessageSchema = z.object({
  role: z.enum(['system', 'user', 'assistant']),
  content: z.string(),
  images: z.array(z.string()).optional(),
});

export const OllamaChatRequestSchema = z.object({
  model: z.string(),
  messages: z.array(OllamaChatMessageSchema),
  format: z.enum(['json']).optional(),
  options: z.object({
    temperature: z.number().min(0).max(2).optional(),
    top_k: z.number().optional(),
    top_p: z.number().min(0).max(1).optional(),
    repeat_penalty: z.number().optional(),
    seed: z.number().optional(),
    num_predict: z.number().optional(),
    stop: z.array(z.string()).optional(),
  }).optional(),
  stream: z.boolean().default(false),
  keep_alive: z.string().optional(),
});

export type OllamaModel = z.infer<typeof OllamaModelSchema>;
export type OllamaGenerateRequest = z.infer<typeof OllamaGenerateRequestSchema>;
export type OllamaGenerateResponse = z.infer<typeof OllamaGenerateResponseSchema>;
export type OllamaChatMessage = z.infer<typeof OllamaChatMessageSchema>;
export type OllamaChatRequest = z.infer<typeof OllamaChatRequestSchema>;

// ============================================================================
// OllamaæœåŠ¡
// ============================================================================

@Injectable()
export class OllamaService implements OnModuleInit, OnModuleDestroy {
  private readonly logger = new Logger(OllamaService.name);
  
  private baseURL: string;
  private enabled: boolean;
  private availableModels: Map<string, OllamaModel> = new Map();
  private modelStats: Map<string, {
    totalRequests: number;
    totalTokens: number;
    averageLatency: number;
    errorCount: number;
    lastUsed: Date;
  }> = new Map();
  
  // è¿æ¥çŠ¶æ€
  private isConnected = false;
  private connectionCheckInterval: NodeJS.Timeout | null = null;
  
  // è¯·æ±‚é˜Ÿåˆ—å’Œé™æµ
  private requestQueue: Array<{
    id: string;
    request: () => Promise<any>;
    resolve: (value: any) => void;
    reject: (error: any) => void;
    timestamp: Date;
  }> = [];
  private processingRequests = 0;
  private maxConcurrentRequests = 5;

  constructor(
    private eventEmitter: EventEmitter2,
  ) {
    this.baseURL = getEnvVar('OLLAMA_BASE_URL', 'http://localhost:11434');
    this.enabled = getBooleanEnvVar('OLLAMA_ENABLED', true);
  }

  async onModuleInit() {
    if (this.enabled) {
      await this.initialize();
    }
  }

  async onModuleDestroy() {
    await this.cleanup();
  }

  /**
   * åˆå§‹åŒ–OllamaæœåŠ¡
   */
  private async initialize(): Promise<void> {
    try {
      this.logger.log('Initializing Ollama service...');
      
      // æ£€æŸ¥OllamaæœåŠ¡è¿æ¥
      await this.checkConnection();
      
      // è·å–å¯ç”¨æ¨¡å‹
      await this.loadAvailableModels();
      
      // å¯åŠ¨è¿æ¥ç›‘æ§
      this.startConnectionMonitoring();
      
      // å¯åŠ¨è¯·æ±‚å¤„ç†å™¨
      this.startRequestProcessor();
      
      this.logger.log(`Ollama service initialized with ${this.availableModels.size} models`);
    } catch (error) {
      this.logger.error('Failed to initialize Ollama service', error);
      this.enabled = false;
    }
  }

  /**
   * æ£€æŸ¥è¿æ¥çŠ¶æ€
   */
  private async checkConnection(): Promise<boolean> {
    try {
      const response = await fetch(`${this.baseURL}/api/version`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000), // 5ç§’è¶…æ—¶
      });
      
      if (response.ok) {
        const version = await response.json();
        this.isConnected = true;
        this.logger.debug(`Connected to Ollama version: ${version.version || 'unknown'}`);
        return true;
      } else {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }
    } catch (error) {
      this.isConnected = false;
      this.logger.warn('Ollama connection failed', error);
      return false;
    }
  }

  /**
   * åŠ è½½å¯ç”¨æ¨¡å‹
   */
  private async loadAvailableModels(): Promise<void> {
    try {
      const response = await fetch(`${this.baseURL}/api/tags`);
      if (!response.ok) {
        throw new Error(`Failed to fetch models: ${response.status}`);
      }
      
      const data = await response.json();
      const models = data.models || [];
      
      this.availableModels.clear();
      
      for (const model of models) {
        const parsedModel = OllamaModelSchema.parse(model);
        this.availableModels.set(parsedModel.name, parsedModel);
        
        // åˆå§‹åŒ–æ¨¡å‹ç»Ÿè®¡
        if (!this.modelStats.has(parsedModel.name)) {
          this.modelStats.set(parsedModel.name, {
            totalRequests: 0,
            totalTokens: 0,
            averageLatency: 0,
            errorCount: 0,
            lastUsed: new Date(),
          });
        }
      }
      
      this.logger.log(`Loaded ${this.availableModels.size} Ollama models: ${Array.from(this.availableModels.keys()).join(', ')}`);
    } catch (error) {
      this.logger.error('Failed to load available models', error);
      throw error;
    }
  }

  /**
   * å¯åŠ¨è¿æ¥ç›‘æ§
   */
  private startConnectionMonitoring(): void {
    this.connectionCheckInterval = setInterval(async () => {
      const wasConnected = this.isConnected;
      await this.checkConnection();
      
      if (wasConnected !== this.isConnected) {
        this.eventEmitter.emit('ollama.connection.changed', {
          connected: this.isConnected,
          timestamp: new Date(),
        });
        
        if (this.isConnected) {
          this.logger.log('Ollama connection restored');
          await this.loadAvailableModels();
        } else {
          this.logger.warn('Ollama connection lost');
        }
      }
    }, 30000); // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
  }

  /**
   * å¯åŠ¨è¯·æ±‚å¤„ç†å™¨
   */
  private startRequestProcessor(): void {
    setInterval(() => {
      this.processRequestQueue();
    }, 100); // æ¯100mså¤„ç†ä¸€æ¬¡é˜Ÿåˆ—
  }

  /**
   * å¤„ç†è¯·æ±‚é˜Ÿåˆ—
   */
  private async processRequestQueue(): Promise<void> {
    if (this.requestQueue.length === 0 || this.processingRequests >= this.maxConcurrentRequests) {
      return;
    }
    
    const availableSlots = this.maxConcurrentRequests - this.processingRequests;
    const requestsToProcess = this.requestQueue.splice(0, availableSlots);
    
    for (const queueItem of requestsToProcess) {
      this.processingRequests++;
      
      queueItem.request()
        .then(result => queueItem.resolve(result))
        .catch(error => queueItem.reject(error))
        .finally(() => {
          this.processingRequests--;
        });
    }
  }

  /**
   * æ·»åŠ è¯·æ±‚åˆ°é˜Ÿåˆ—
   */
  private queueRequest<T>(request: () => Promise<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      this.requestQueue.push({
        id: crypto.randomUUID(),
        request,
        resolve,
        reject,
        timestamp: new Date(),
      });
    });
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬
   */
  async generate(request: OllamaGenerateRequest): Promise<OllamaGenerateResponse> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    if (!this.availableModels.has(request.model)) {
      throw new Error(`Model ${request.model} is not available`);
    }
    
    return this.queueRequest(async () => {
      const startTime = Date.now();
      
      try {
        this.logger.debug(`Generating text with model: ${request.model}`);
        
        const response = await withTimeout(
          fetch(`${this.baseURL}/api/generate`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(request),
          }),
          60000 // 60ç§’è¶…æ—¶
        );
        
        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        const result = await response.json();
        const parsedResult = OllamaGenerateResponseSchema.parse(result);
        
        const latency = Date.now() - startTime;
        this.updateModelStats(request.model, latency, parsedResult.eval_count || 0, false);
        
        this.eventEmitter.emit('ollama.generate.completed', {
          model: request.model,
          latency,
          tokens: parsedResult.eval_count,
        });
        
        return parsedResult;
      } catch (error) {
        const latency = Date.now() - startTime;
        this.updateModelStats(request.model, latency, 0, true);
        
        this.logger.error(`Generate request failed for model ${request.model}`, error);
        throw error;
      }
    });
  }

  /**
   * èŠå¤©å¯¹è¯
   */
  async chat(request: OllamaChatRequest): Promise<OllamaGenerateResponse> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    if (!this.availableModels.has(request.model)) {
      throw new Error(`Model ${request.model} is not available`);
    }
    
    return this.queueRequest(async () => {
      const startTime = Date.now();
      
      try {
        this.logger.debug(`Chat request with model: ${request.model}`);
        
        const response = await withTimeout(
          fetch(`${this.baseURL}/api/chat`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(request),
          }),
          60000 // 60ç§’è¶…æ—¶
        );
        
        if (!response.ok) {
          throw new Error(`HTTP ${response.status}: ${response.statusText}`);
        }
        
        const result = await response.json();
        const parsedResult = OllamaGenerateResponseSchema.parse(result);
        
        const latency = Date.now() - startTime;
        this.updateModelStats(request.model, latency, parsedResult.eval_count || 0, false);
        
        this.eventEmitter.emit('ollama.chat.completed', {
          model: request.model,
          latency,
          tokens: parsedResult.eval_count,
        });
        
        return parsedResult;
      } catch (error) {
        const latency = Date.now() - startTime;
        this.updateModelStats(request.model, latency, 0, true);
        
        this.logger.error(`Chat request failed for model ${request.model}`, error);
        throw error;
      }
    });
  }

  /**
   * æµå¼ç”Ÿæˆæ–‡æœ¬
   */
  async *generateStream(request: OllamaGenerateRequest): AsyncGenerator<string, void, unknown> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    if (!this.availableModels.has(request.model)) {
      throw new Error(`Model ${request.model} is not available`);
    }
    
    const streamRequest = { ...request, stream: true };
    
    try {
      this.logger.debug(`Streaming text generation with model: ${request.model}`);
      
      const response = await fetch(`${this.baseURL}/api/generate`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(streamRequest),
      });
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }
      
      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('Failed to get response reader');
      }
      
      const decoder = new TextDecoder();
      
      try {
        while (true) {
          const { done, value } = await reader.read();
          
          if (done) break;
          
          const chunk = decoder.decode(value, { stream: true });
          const lines = chunk.split('\n').filter(line => line.trim());
          
          for (const line of lines) {
            try {
              const data = JSON.parse(line);
              if (data.response) {
                yield data.response;
              }
              
              if (data.done) {
                return;
              }
            } catch (parseError) {
              this.logger.warn('Failed to parse streaming response line', parseError);
            }
          }
        }
      } finally {
        reader.releaseLock();
      }
    } catch (error) {
      this.logger.error(`Stream generation failed for model ${request.model}`, error);
      throw error;
    }
  }

  /**
   * æ‹‰å–æ¨¡å‹
   */
  async pullModel(modelName: string): Promise<void> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    try {
      this.logger.log(`Pulling model: ${modelName}`);
      
      const response = await fetch(`${this.baseURL}/api/pull`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name: modelName }),
      });
      
      if (!response.ok) {
        throw new Error(`Failed to pull model: ${response.status}`);
      }
      
      // æ‹‰å–å®Œæˆåé‡æ–°åŠ è½½æ¨¡å‹åˆ—è¡¨
      await this.loadAvailableModels();
      
      this.eventEmitter.emit('ollama.model.pulled', {
        model: modelName,
        timestamp: new Date(),
      });
      
      this.logger.log(`Successfully pulled model: ${modelName}`);
    } catch (error) {
      this.logger.error(`Failed to pull model ${modelName}`, error);
      throw error;
    }
  }

  /**
   * åˆ é™¤æ¨¡å‹
   */
  async deleteModel(modelName: string): Promise<void> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    try {
      this.logger.log(`Deleting model: ${modelName}`);
      
      const response = await fetch(`${this.baseURL}/api/delete`, {
        method: 'DELETE',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name: modelName }),
      });
      
      if (!response.ok) {
        throw new Error(`Failed to delete model: ${response.status}`);
      }
      
      // ä»æœ¬åœ°ç¼“å­˜ä¸­ç§»é™¤
      this.availableModels.delete(modelName);
      this.modelStats.delete(modelName);
      
      this.eventEmitter.emit('ollama.model.deleted', {
        model: modelName,
        timestamp: new Date(),
      });
      
      this.logger.log(`Successfully deleted model: ${modelName}`);
    } catch (error) {
      this.logger.error(`Failed to delete model ${modelName}`, error);
      throw error;
    }
  }

  /**
   * è·å–æ¨¡å‹ä¿¡æ¯
   */
  async getModelInfo(modelName: string): Promise<any> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    try {
      const response = await fetch(`${this.baseURL}/api/show`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ name: modelName }),
      });
      
      if (!response.ok) {
        throw new Error(`Failed to get model info: ${response.status}`);
      }
      
      return await response.json();
    } catch (error) {
      this.logger.error(`Failed to get model info for ${modelName}`, error);
      throw error;
    }
  }

  /**
   * ç”ŸæˆåµŒå…¥å‘é‡
   */
  async generateEmbeddings(model: string, prompt: string): Promise<number[]> {
    if (!this.enabled || !this.isConnected) {
      throw new Error('Ollama service is not available');
    }
    
    return this.queueRequest(async () => {
      try {
        this.logger.debug(`Generating embeddings with model: ${model}`);
        
        const response = await fetch(`${this.baseURL}/api/embeddings`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ model, prompt }),
        });
        
        if (!response.ok) {
          throw new Error(`Failed to generate embeddings: ${response.status}`);
        }
        
        const result = await response.json();
        return result.embedding || [];
      } catch (error) {
        this.logger.error(`Failed to generate embeddings with model ${model}`, error);
        throw error;
      }
    });
  }

  /**
   * æ›´æ–°æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
   */
  private updateModelStats(
    modelName: string, 
    latency: number, 
    tokens: number, 
    isError: boolean
  ): void {
    const stats = this.modelStats.get(modelName);
    if (!stats) return;
    
    stats.totalRequests++;
    stats.lastUsed = new Date();
    
    if (isError) {
      stats.errorCount++;
    } else {
      stats.totalTokens += tokens;
      
      // æ›´æ–°å¹³å‡å»¶è¿Ÿ
      const totalLatency = stats.averageLatency * (stats.totalRequests - 1) + latency;
      stats.averageLatency = totalLatency / stats.totalRequests;
    }
  }

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  getAvailableModels(): OllamaModel[] {
    return Array.from(this.availableModels.values());
  }

  /**
   * æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
   */
  isModelAvailable(modelName: string): boolean {
    return this.availableModels.has(modelName);
  }

  /**
   * è·å–æœåŠ¡çŠ¶æ€
   */
  getStatus() {
    return {
      enabled: this.enabled,
      connected: this.isConnected,
      baseURL: this.baseURL,
      availableModels: Array.from(this.availableModels.keys()),
      queueSize: this.requestQueue.length,
      processingRequests: this.processingRequests,
      maxConcurrentRequests: this.maxConcurrentRequests,
    };
  }

  /**
   * è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
   */
  getModelStats(modelName?: string) {
    if (modelName) {
      return this.modelStats.get(modelName) || null;
    }
    
    return Object.fromEntries(this.modelStats);
  }

  /**
   * è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
   */
  getServiceStats() {
    const totalRequests = Array.from(this.modelStats.values())
      .reduce((sum, stats) => sum + stats.totalRequests, 0);
    
    const totalTokens = Array.from(this.modelStats.values())
      .reduce((sum, stats) => sum + stats.totalTokens, 0);
    
    const totalErrors = Array.from(this.modelStats.values())
      .reduce((sum, stats) => sum + stats.errorCount, 0);
    
    const averageLatency = Array.from(this.modelStats.values())
      .reduce((sum, stats) => sum + stats.averageLatency, 0) / this.modelStats.size;
    
    return {
      totalRequests,
      totalTokens,
      totalErrors,
      averageLatency: averageLatency || 0,
      errorRate: totalRequests > 0 ? totalErrors / totalRequests : 0,
      modelsCount: this.availableModels.size,
      queueSize: this.requestQueue.length,
      processingRequests: this.processingRequests,
    };
  }

  /**
   * æ¸…ç†èµ„æº
   */
  private async cleanup(): Promise<void> {
    this.logger.log('Cleaning up Ollama service...');
    
    if (this.connectionCheckInterval) {
      clearInterval(this.connectionCheckInterval);
    }
    
    // æ¸…ç©ºè¯·æ±‚é˜Ÿåˆ—
    this.requestQueue.length = 0;
    
    this.logger.log('Ollama service cleaned up');
  }
}