import { Trace } from '@juanie/core/observability'
import { Injectable, type OnModuleInit } from '@nestjs/common'
import { ConfigService } from '@nestjs/config'
import { PinoLogger } from 'nestjs-pino'
import { Ollama } from 'ollama'

@Injectable()
export class OllamaService implements OnModuleInit {
  private ollama: Ollama
  private isConnected = false

  constructor(
    config: ConfigService,
    private readonly logger: PinoLogger,
  ) {
    this.logger.setContext(OllamaService.name)
    this.ollama = new Ollama({
      host: config.get('OLLAMA_HOST') || 'http://localhost:11434',
    })
  }

  async onModuleInit() {
    await this.checkConnection()
    await this.ensureModelsAvailable()
  }

  // æ£€æŸ¥è¿æ¥
  private async checkConnection() {
    try {
      await this.ollama.list()
      this.isConnected = true
      this.logger.info('âœ… Ollama è¿æ¥æˆåŠŸ')
    } catch {
      this.isConnected = false
      this.logger.warn('âš ï¸ Ollama è¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå“åº”')
      this.logger.warn('å¯åŠ¨ Ollama: docker-compose up -d ollama')
    }
  }

  // ç¡®ä¿åŸºç¡€æ¨¡å‹å¯ç”¨
  private async ensureModelsAvailable() {
    if (!this.isConnected) return

    try {
      const models = await this.listModels()
      const modelNames = models.map((m) => m.name)

      // æ¨èçš„è½»é‡çº§æ¨¡å‹
      const recommendedModels = [
        'llama3.2:3b', // 3B å‚æ•°ï¼Œé€‚åˆä»£ç å’Œå¯¹è¯
        'codellama:7b', // 7B å‚æ•°ï¼Œä¸“é—¨ç”¨äºä»£ç 
        'mistral:7b', // 7B å‚æ•°ï¼Œé€šç”¨æ¨¡å‹
      ]

      const missingModels = recommendedModels.filter((model) => {
        const modelPrefix = model.split(':')[0]
        if (!modelPrefix) return false
        return !modelNames.some((name) => name.startsWith(modelPrefix))
      })

      if (missingModels.length > 0) {
        this.logger.info('ğŸ“¥ æ¨èä¸‹è½½ä»¥ä¸‹æ¨¡å‹ä»¥è·å¾—æœ€ä½³ä½“éªŒ:')
        missingModels.forEach((model) => {
          this.logger.info(`   ollama pull ${model}`)
        })
      }
    } catch (error) {
      const message = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      this.logger.warn(`æ£€æŸ¥æ¨¡å‹æ—¶å‡ºé”™: ${message}`)
    }
  }

  // ç”Ÿæˆå“åº”
  @Trace('ollama.generate')
  async generate(
    model: string,
    prompt: string,
    system?: string,
    options?: {
      temperature?: number
      max_tokens?: number
    },
  ): Promise<string> {
    if (!this.isConnected) {
      return this.generateMockResponse(model, prompt, system)
    }

    try {
      const response = await this.ollama.generate({
        model,
        prompt,
        system,
        stream: false,
        options: {
          temperature: options?.temperature || 0.7,
          num_predict: options?.max_tokens || 2048,
        },
      })
      return response.response
    } catch (error) {
      this.logger.error('Ollama ç”Ÿæˆé”™è¯¯', error)
      return this.generateMockResponse(model, prompt, system)
    }
  }

  // æµå¼å“åº”
  @Trace('ollama.generateStream')
  async *generateStream(
    model: string,
    prompt: string,
    system?: string,
    options?: {
      temperature?: number
      max_tokens?: number
    },
  ): AsyncGenerator<string, void, unknown> {
    if (!this.isConnected) {
      yield* this.generateMockStream(model, prompt, system)
      return
    }

    try {
      const stream = await this.ollama.generate({
        model,
        prompt,
        system,
        stream: true,
        options: {
          temperature: options?.temperature || 0.7,
          num_predict: options?.max_tokens || 2048,
        },
      })

      for await (const chunk of stream) {
        if (chunk.response) {
          yield chunk.response
        }
      }
    } catch (error) {
      this.logger.error('Ollama æµå¼ç”Ÿæˆé”™è¯¯', error)
      yield* this.generateMockStream(model, prompt, system)
    }
  }

  // å¯¹è¯ï¼ˆå¸¦å†å²ï¼‰
  @Trace('ollama.chat')
  async chat(
    model: string,
    messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>,
    options?: {
      temperature?: number
      max_tokens?: number
    },
  ): Promise<string> {
    if (!this.isConnected) {
      const lastMessage = messages[messages.length - 1]
      if (!lastMessage) {
        throw new Error('No messages provided')
      }
      const systemMessage = messages.find((m) => m.role === 'system')
      return this.generateMockResponse(model, lastMessage.content, systemMessage?.content)
    }

    try {
      const response = await this.ollama.chat({
        model,
        messages,
        stream: false,
        options: {
          temperature: options?.temperature || 0.7,
          num_predict: options?.max_tokens || 2048,
        },
      })
      return response.message.content
    } catch (error) {
      this.logger.error('Ollama å¯¹è¯é”™è¯¯', error)
      const lastMessage = messages[messages.length - 1]
      if (!lastMessage) {
        throw new Error('No messages provided')
      }
      const systemMessage = messages.find((m) => m.role === 'system')
      return this.generateMockResponse(model, lastMessage.content, systemMessage?.content)
    }
  }

  // åˆ—å‡ºå¯ç”¨æ¨¡å‹
  @Trace('ollama.listModels')
  async listModels() {
    if (!this.isConnected) {
      return [
        { name: 'llama3.2:3b', size: 2000000000, modified: new Date() },
        { name: 'codellama:7b', size: 4000000000, modified: new Date() },
        { name: 'mistral:7b', size: 4000000000, modified: new Date() },
      ]
    }

    try {
      const models = await this.ollama.list()
      return models.models.map((m) => ({
        name: m.name,
        size: m.size,
        modified: new Date(m.modified_at),
      }))
    } catch (error) {
      this.logger.error('è·å–æ¨¡å‹åˆ—è¡¨é”™è¯¯', error)
      return []
    }
  }

  // æ‹‰å–æ¨¡å‹
  @Trace('ollama.pullModel')
  async pullModel(model: string): Promise<void> {
    if (!this.isConnected) {
      throw new Error('Ollama æœªè¿æ¥')
    }

    try {
      this.logger.info(`ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: ${model}`)
      await this.ollama.pull({ model })
      this.logger.info(`âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: ${model}`)
    } catch (error) {
      this.logger.error(`æ¨¡å‹ä¸‹è½½å¤±è´¥: ${model}`, error)
      throw error
    }
  }

  // åˆ é™¤æ¨¡å‹
  @Trace('ollama.deleteModel')
  async deleteModel(model: string): Promise<void> {
    if (!this.isConnected) {
      throw new Error('Ollama æœªè¿æ¥')
    }

    try {
      await this.ollama.delete({ model })
      this.logger.info(`ğŸ—‘ï¸ æ¨¡å‹å·²åˆ é™¤: ${model}`)
    } catch (error) {
      this.logger.error(`åˆ é™¤æ¨¡å‹å¤±è´¥: ${model}`, error)
      throw error
    }
  }

  // æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
  @Trace('ollama.modelExists')
  async modelExists(model: string): Promise<boolean> {
    const models = await this.listModels()
    const modelPrefix = model.split(':')[0]
    if (!modelPrefix) return false
    return models.some((m) => m.name === model || m.name.startsWith(modelPrefix))
  }

  // è·å–æ¨èæ¨¡å‹
  getRecommendedModels() {
    return [
      {
        name: 'llama3.2:3b',
        description: 'è½»é‡çº§é€šç”¨æ¨¡å‹ï¼Œé€‚åˆå¯¹è¯å’Œç®€å•ä»»åŠ¡',
        size: '2GB',
        use_case: ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'ç®€å•é—®ç­”'],
      },
      {
        name: 'codellama:7b',
        description: 'ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆå’Œä»£ç å®¡æŸ¥',
        size: '4GB',
        use_case: ['ä»£ç ç”Ÿæˆ', 'ä»£ç å®¡æŸ¥', 'ä»£ç è§£é‡Š'],
      },
      {
        name: 'mistral:7b',
        description: 'é«˜è´¨é‡é€šç”¨æ¨¡å‹ï¼Œå¹³è¡¡æ€§èƒ½å’Œè´¨é‡',
        size: '4GB',
        use_case: ['å¤æ‚å¯¹è¯', 'åˆ†æ', 'åˆ›ä½œ'],
      },
    ]
  }

  // ç”Ÿæˆæ¨¡æ‹Ÿå“åº”ï¼ˆå½“ Ollama ä¸å¯ç”¨æ—¶ï¼‰
  private generateMockResponse(model: string, prompt: string, system?: string): string {
    const responses = {
      'code-reviewer': [
        'ä»£ç å®¡æŸ¥å»ºè®®ï¼š\n1. å»ºè®®æ·»åŠ é”™è¯¯å¤„ç†\n2. è€ƒè™‘æ€§èƒ½ä¼˜åŒ–\n3. ä»£ç é£æ ¼ç¬¦åˆè§„èŒƒ',
        'è¿™æ®µä»£ç çœ‹èµ·æ¥ä¸é”™ï¼å»ºè®®ï¼š\n- æ·»åŠ å•å…ƒæµ‹è¯•\n- è€ƒè™‘è¾¹ç•Œæƒ…å†µå¤„ç†\n- æ–‡æ¡£å¯ä»¥æ›´è¯¦ç»†',
        'å‘ç°å‡ ä¸ªæ”¹è¿›ç‚¹ï¼š\n1. å˜é‡å‘½åå¯ä»¥æ›´æ¸…æ™°\n2. å‡½æ•°å¯ä»¥æ‹†åˆ†å¾—æ›´å°\n3. æ·»åŠ ç±»å‹æ³¨è§£',
      ],
      'devops-engineer': [
        'DevOps å»ºè®®ï¼š\n1. éƒ¨ç½²ç­–ç•¥å»ºè®®ä½¿ç”¨è“ç»¿éƒ¨ç½²\n2. å»ºè®®æ·»åŠ å¥åº·æ£€æŸ¥\n3. è€ƒè™‘æ·»åŠ è‡ªåŠ¨å›æ»šæœºåˆ¶',
        'åŸºç¡€è®¾æ–½å»ºè®®ï¼š\n- ä½¿ç”¨ Kubernetes è¿›è¡Œå®¹å™¨ç¼–æ’\n- é…ç½®ç›‘æ§å’Œå‘Šè­¦\n- å®æ–½ GitOps å·¥ä½œæµ',
        'å®‰å…¨å»ºè®®ï¼š\n1. å¯ç”¨ RBAC æƒé™æ§åˆ¶\n2. ä½¿ç”¨ Secret ç®¡ç†æ•æ„Ÿä¿¡æ¯\n3. å®šæœŸæ›´æ–°ä¾èµ–',
      ],
      'cost-optimizer': [
        'æˆæœ¬ä¼˜åŒ–å»ºè®®ï¼š\n1. å¯ä»¥ä½¿ç”¨ Spot å®ä¾‹èŠ‚çœ 70% æˆæœ¬\n2. å»ºè®®å¯ç”¨è‡ªåŠ¨æ‰©ç¼©å®¹\n3. ä¼˜åŒ–å­˜å‚¨ä½¿ç”¨',
        'èµ„æºä¼˜åŒ–ï¼š\n- å³è°ƒå®ä¾‹å¤§å°\n- ä½¿ç”¨é¢„ç•™å®ä¾‹\n- æ¸…ç†æœªä½¿ç”¨çš„èµ„æº',
        'æˆæœ¬åˆ†æï¼š\n1. å½“å‰æˆæœ¬ä¸»è¦åœ¨è®¡ç®—èµ„æº\n2. å»ºè®®ä½¿ç”¨æ›´ä¾¿å®œçš„å­˜å‚¨ç±»å‹\n3. è€ƒè™‘å¤šäº‘ç­–ç•¥',
      ],
    }

    // æ ¹æ® system prompt é€‰æ‹©å“åº”ç±»å‹
    let responseType = 'general'
    if (system?.includes('code') || system?.includes('ä»£ç ')) {
      responseType = 'code-reviewer'
    } else if (system?.includes('devops') || system?.includes('éƒ¨ç½²')) {
      responseType = 'devops-engineer'
    } else if (system?.includes('cost') || system?.includes('æˆæœ¬')) {
      responseType = 'cost-optimizer'
    }

    const typeResponses = responses[responseType as keyof typeof responses]
    if (typeResponses) {
      const randomResponse = typeResponses[Math.floor(Math.random() * typeResponses.length)]
      return `${randomResponse}\n\né’ˆå¯¹æ‚¨çš„é—®é¢˜"${prompt}"ï¼Œè¿™æ˜¯åŸºäº ${model} æ¨¡å‹çš„å»ºè®®ã€‚\n\nâš ï¸ å½“å‰ä½¿ç”¨æ¨¡æ‹Ÿå“åº”ï¼Œå¯åŠ¨ Ollama è·å¾—çœŸå® AI å›ç­”ã€‚`
    }

    return `æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯ï¼š"${prompt}"\n\nè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”ã€‚è¦è·å¾—çœŸå®çš„ AI å›ç­”ï¼Œè¯·ï¼š\n1. å¯åŠ¨ Ollama: docker-compose up -d ollama\n2. ä¸‹è½½æ¨¡å‹: ollama pull ${model}\n3. é‡å¯åº”ç”¨\n\næ¨¡å‹: ${model}`
  }

  // ç”Ÿæˆæ¨¡æ‹Ÿæµå¼å“åº”
  private async *generateMockStream(
    model: string,
    prompt: string,
    system?: string,
  ): AsyncGenerator<string, void, unknown> {
    const response = this.generateMockResponse(model, prompt, system)
    const words = response.split(' ')

    for (const word of words) {
      yield `${word} `
      await new Promise((resolve) => setTimeout(resolve, 50)) // æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ
    }
  }

  // è·å–è¿æ¥çŠ¶æ€
  isOllamaConnected(): boolean {
    return this.isConnected
  }
}
